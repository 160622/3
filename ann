import numpy as np 

# Input and Output
x = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)

print('Input:\n', x)

# Normalize inputs and outputs
x = x / np.amax(x, axis=0)
y = y / 100

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def derivative_sigmoid(x):
    return x * (1 - x)

# Parameters
epoch = 7000
lr = 0.1
input_units = 2
hidden_units = 3
output_units = 1

# Initialize weights and biases
wh = np.random.uniform(size=(input_units, hidden_units))
bh = np.random.uniform(size=(1, hidden_units))
wout = np.random.uniform(size=(hidden_units, output_units))
bout = np.random.uniform(size=(1, output_units))

# Training loop
for i in range(epoch):
    # Forward pass
    hinp = np.dot(x, wh) + bh
    hout = sigmoid(hinp)
    outinp = np.dot(hout, wout) + bout
    output = sigmoid(outinp)

    # Backpropagation
    EO = y - output
    ogradient = derivative_sigmoid(output)
    doutput = EO * ogradient

    EH = doutput.dot(wout.T)
    hgradient = derivative_sigmoid(hout)
    dhidden = EH * hgradient

    # Update weights and biases
    wout += hout.T.dot(doutput) * lr
    wh += x.T.dot(dhidden) * lr

print("Normalized Input:\n", x)
print("Actual Output:\n", y)
print("Predicted Output:\n", output)
